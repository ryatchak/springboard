{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import re\n",
    "import numpy as np\n",
    "import itertools\n",
    "import capstone2_utilities as cs2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "Collecting all the major steps I took to go from `Tweets.csv` from the [US Airline sentiment](https://www.kaggle.com/crowdflower/twitter-airline-sentiment#Tweets.csv) dataset available on Kaggle, to the file `tweets_with_originals.csv` that I use for sentiment analysis and the dataset data story. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to the dataset\n",
    "A look at the essential stats about our dataset, and a discussion of what possible issues were noticed during exploratory analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 15)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('Tweets.csv', parse_dates = ['tweet_created'])\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52-08:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59-08:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48-08:00</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36-08:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45-08:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "              tweet_created tweet_location               user_timezone  \n",
       "0 2015-02-24 11:35:52-08:00            NaN  Eastern Time (US & Canada)  \n",
       "1 2015-02-24 11:15:59-08:00            NaN  Pacific Time (US & Canada)  \n",
       "2 2015-02-24 11:15:48-08:00      Lets Play  Central Time (US & Canada)  \n",
       "3 2015-02-24 11:15:36-08:00            NaN  Pacific Time (US & Canada)  \n",
       "4 2015-02-24 11:14:45-08:00            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet_id', 'airline_sentiment', 'airline_sentiment_confidence',\n",
       "       'negativereason', 'negativereason_confidence', 'airline',\n",
       "       'airline_sentiment_gold', 'name', 'negativereason_gold',\n",
       "       'retweet_count', 'text', 'tweet_coord', 'tweet_created',\n",
       "       'tweet_location', 'user_timezone'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14640 entries, 0 to 14639\n",
      "Data columns (total 15 columns):\n",
      "tweet_id                        14640 non-null int64\n",
      "airline_sentiment               14640 non-null object\n",
      "airline_sentiment_confidence    14640 non-null float64\n",
      "negativereason                  9178 non-null object\n",
      "negativereason_confidence       10522 non-null float64\n",
      "airline                         14640 non-null object\n",
      "airline_sentiment_gold          40 non-null object\n",
      "name                            14640 non-null object\n",
      "negativereason_gold             32 non-null object\n",
      "retweet_count                   14640 non-null int64\n",
      "text                            14640 non-null object\n",
      "tweet_coord                     1019 non-null object\n",
      "tweet_created                   14640 non-null datetime64[ns, pytz.FixedOffset(-480)]\n",
      "tweet_location                  9907 non-null object\n",
      "user_timezone                   9820 non-null object\n",
      "dtypes: datetime64[ns, pytz.FixedOffset(-480)](1), float64(2), int64(2), object(10)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "tweets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that there aren't a lot of gold standard sentiments/ negative reasons. This isn't at all surprising, but just something to keep in mind when we draw conclusions at the end of the project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML-general text\n",
    "While looking at tweet lengths, I saw that some tweets were longer than 140 characters. These tweets contain HTML artifacts such as \"&amp\" and \"&gt\". We'll take care of these using the html parser from BeautifulSoup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@VirginAmerica hi! i'm so excited about your $99 LGA-&gt;DAL deal- but i've been trying 2 book since last week &amp; the page never loads. thx!\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.loc[55].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def parse_html(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    return soup.get_text()\n",
    "text_parsed=tweets.text.map(parse_html)\n",
    "tweets['text'] = text_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@VirginAmerica hi! i'm so excited about your $99 LGA->DAL deal- but i've been trying 2 book since last week & the page never loads. thx!\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.loc[55].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_appear = tweets.tweet_id.value_counts()\n",
    "duplicate_tweets = no_appear.where(no_appear > 1).dropna()\n",
    "df_duplicate = tweets[tweets.tweet_id.isin(duplicate_tweets.index)].sort_values(by='tweet_id')\n",
    "duplicate_ids = list(duplicate_tweets.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 155 duplicate tweets. It's a bit hard to tell what went on with these tweets but the sentiment confidence values can be different for the same tweet (in fact, they often are). \n",
    "What we care about the most are the positive/negative/neutral labels for airline sentiment. So for now, my solution is to aggregate the entries. For the sentiment_confidence and reason_confidence values, we'll take the mean. For negative reason we'll take a join so that we don't lose information. \n",
    "\n",
    "We'll refactor the duplicates, drop from tweets, and then concatenate the fixed entries. \n",
    "\n",
    "For the ones with two different sentiment labels we'll have to decide what to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18,)\n"
     ]
    }
   ],
   "source": [
    "no_different_sentiments = df_duplicate.groupby('tweet_id').airline_sentiment.apply(set).apply(len)\n",
    "print(no_different_sentiments.where(no_different_sentiments > 1).dropna().shape)\n",
    "drop_list = list(no_different_sentiments.where(no_different_sentiments > 1).dropna().index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are only 18 tweets, we'll just drop them from consideration. \n",
    "\n",
    "For the rest of the tweets, we need to group them and aggregate the entries as planned. The utility function to do this, `combine_duplicate_tweets`, is in `capstone2_utilities`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rika/code/springboard/capstone2_nlp/capstone2_utilities.py:26: RuntimeWarning: Mean of empty slice\n",
      "  row_dict['negativereason_confidence'] = np.nanmean(one_tweet_df['negativereason_confidence'])\n"
     ]
    }
   ],
   "source": [
    "df_duplicate = df_duplicate[~df_duplicate.tweet_id.isin(drop_list)]\n",
    "refactored_duplicates = df_duplicate.groupby('tweet_id').apply(cs2.combine_duplicate_tweets)\n",
    "refactored_duplicates = refactored_duplicates.droplevel(level=0).reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(refactored_duplicates.tweet_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge the refactored duplicates back in to the dataframe and make sure that the tweet_ids are unique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = tweets.loc[~tweets.tweet_id.isin(duplicate_ids)]\n",
    "tweets = pd.concat([tweets, refactored_duplicates], axis = 0, ignore_index = True)\n",
    "len(tweets.tweet_id.unique()) ==  tweets.tweet_id.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recovering post-processed tweet text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An open question is how post-processed the `text` column of `Tweets.csv` is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I ‚ù§Ô∏è flying @VirginAmerica. ‚ò∫Ô∏èüëç'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.text.iloc[18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emojis are definitely present. Further examination shows that emails also haven't been cleaned. Some of the emails are personal emails. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@VirginAmerica FYI the info@virginamerica.com email address you say to contact in password reset emails doesn't exist. Emails bounce.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regex pattern to find emails in tweet text: \n",
    "#tweets.text.loc[tweets.text.str.contains('\\w+@\\w+')]\n",
    "# an example of a (non-private) email identified using this regex pattern\n",
    "tweets.text.loc[407]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While making text clouds of common words in the dataset (viewable in the data story notebook), I saw that there was indeed some post-processing on the tweet text. \n",
    "\n",
    "Words such as \"late\" have been replaced with \"Late Flight\", the stem \"cancell-\" has been replaced with \"cancellation\", etc. This appears to have been done to increase the performance of the \"negative reason\" classifier. \n",
    "\n",
    "The substitution also caught some undesired words: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@VirginAmerica you got cheese pLate Flights too.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.text.loc[tweets.text.str.contains('Late Flight')].loc[473]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@VirginAmerica is flight 882 Cancelled Flightled and what do I do if it is?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.iloc[129].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is no further information about post-processing that took place on this dataset, I made the decision to try to obtain the original text of all tweets in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1a: Retrieve original tweet text using Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter\n",
    "# CONSUMER_KEY, etc are private, you can get your own \n",
    "# if you sign up for a twitter developer acct. \n",
    "\n",
    "api = twitter.Api(consumer_key=CONSUMER_KEY,\n",
    "                      consumer_secret=CONSUMER_SECRET,\n",
    "                      access_token_key=ACCESS_TOKEN,\n",
    "                      access_token_secret=ACCESS_TOKEN_SECRET, \n",
    "                      sleep_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_tweet_text = tweets[['tweet_id']]\n",
    "original_tweet_text = original_tweet_text.set_index('tweet_id')\n",
    "original_tweet_text['text'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 3.1524418300000434\n",
      "2000 6.327197419000072\n",
      "3000 9.50261570400005\n",
      "4000 12.785656279000023\n",
      "5000 15.841215282999997\n",
      "6000 19.029530859000033\n",
      "7000 22.095422701000075\n",
      "8000 25.175771483000062\n",
      "9000 28.37528906099999\n",
      "10000 31.548471812000003\n",
      "11000 34.76600799000005\n",
      "12000 37.98337522600002\n",
      "13000 41.10188202200004\n",
      "14000 44.33335429099998\n",
      "46.023541120999994\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "start_time = time.monotonic()\n",
    "for rng in range(100, original_tweet_text.shape[0], 100):\n",
    "    if rng % 1000 == 0: \n",
    "        print(rng, time.monotonic() - start_time)\n",
    "    input_list = list(original_tweet_text.index[rng - 100: rng])\n",
    "    request_out = api.GetStatuses(input_list)\n",
    "    for status in request_out:\n",
    "        original_tweet_text.loc[status.id, 'text'] = status.text\n",
    "end_time = time.monotonic()\n",
    "elapsed_time = end_time - start_time \n",
    "print(elapsed_time)\n",
    "original_tweet_text.to_csv('original_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_tweet_text = pd.read_csv('original_tweets.csv')\n",
    "tweets_with_originals = tweets.merge(original_tweet_text, on = 'tweet_id', suffixes = ['_proc', '_orig'])\n",
    "missing_orig_tweets = tweets_with_originals[tweets_with_originals.text_orig.isna()][['tweet_id']]\n",
    "missing_orig_tweets['text'] = ''\n",
    "missing_orig_tweets = missing_orig_tweets.set_index('tweet_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This datasest was originally obtained in 2015. It's not surprising that many of the tweets are no longer available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3197, 16)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_with_originals[tweets_with_originals.text_orig.isna()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1b: Backsubstitute text for tweets that are no longer available. \n",
    "Since it looks like the text has not really been altered much otherwise, we'll backsubstitute the text in the rest of the tweets and regard this as \"original text\". \n",
    "\n",
    "It seems that the substitutions we see come from the negative reasons in the `negativereason` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_reason_list = list(tweets_with_originals.negativereason.str.split(',').values)\n",
    "neg_reason_list = [item for item in neg_reason_list if type(item) == list]\n",
    "neg_reason_list = [item for sublist in neg_reason_list for item in sublist]\n",
    "negative_reasons = set(item for item in neg_reason_list)\n",
    "\n",
    "from collections import defaultdict\n",
    "neg_reasons_dict = defaultdict(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which negative reasons really show up in the twitter text. \n",
    "\n",
    "We will assume that capitalization in this fashion comes from the replacement: of course, there is some risk that we're substituting perfectly accurate tweets: however, it's unlikely as social media users tend not to capitalize in this fashion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica hi! I just bked a cool birthday trip with you, but i can't add my elevate no. cause i entered my middle name during Flight Booking Problems üò¢\n",
      "@VirginAmerica you got cheese pLate Flights too.\n",
      "@VirginAmerica is flight 882 Cancelled Flightled and what do I do if it is?\n"
     ]
    }
   ],
   "source": [
    "for elt in tweets_with_originals.loc[[30, 473, 129]].text_proc.values:\n",
    "    print(elt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Late Flight', 97),\n",
       " ('Cancelled Flight', 221),\n",
       " (' Flight Booking Problems', 18),\n",
       " (' Cancelled Flight', 220),\n",
       " (' Late Flight', 80),\n",
       " ('Flight Booking Problems', 28)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_orig = tweets_with_originals.loc[tweets_with_originals.text_orig.isna()].tweet_id.values\n",
    "tweets_with_originals = tweets_with_originals.set_index('tweet_id')\n",
    "for reason in negative_reasons: \n",
    "    reason_bool = tweets_with_originals.loc[missing_orig].text_proc.str.contains(reason)\n",
    "    neg_reasons_dict[reason] = list(reason_bool.where(reason_bool == True).dropna().index)\n",
    "[(i, len(neg_reasons_dict[i])) for i in neg_reasons_dict.keys() if len(neg_reasons_dict[i])!=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at examples for each of the instances above, I built the following dict of replacements: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_map = {'Cancelled Flight': 'cancell', 'Late Flight': 'late', 'Flight Booking Problems':'booking'}\n",
    "tweets_with_originals.at[missing_orig, 'text_orig'] = tweets_with_originals.loc[missing_orig, 'text_proc']\n",
    "for proc_word, orig_word in backward_map.items():\n",
    "    tweets_with_originals.loc[missing_orig, 'text_orig'] = tweets_with_originals.loc[missing_orig,'text_orig'].str.replace(r'{}'.format(proc_word), r'{}'.format(orig_word), regex=True)\n",
    "tweets_with_originals = tweets_with_originals.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We confirm that it worked by checking our example tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica hi! I just bked a cool birthday trip with you, but i can't add my elevate no. cause i entered my middle name during booking üò¢\n",
      "@VirginAmerica you got cheese plates too.\n",
      "@VirginAmerica is flight 882 cancelled and what do I do if it is?\n"
     ]
    }
   ],
   "source": [
    "for elt in tweets_with_originals.loc[[30, 473, 129]].text_orig.values:\n",
    "    print(elt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timezone Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some nonstandard timezones in the dataset. The timezones allowed in Twitter come from Ruby on Rails `ActiveSupport::TimeZone`. Mapping to standardized timezones are [here](https://api.rubyonrails.org/classes/ActiveSupport/TimeZone.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "timezone_dict = {}\n",
    "with open('rails_timezones.txt', 'r') as file: \n",
    "    for line in file.readlines():\n",
    "        nonstd, std = line.split('\\t')\n",
    "        timezone_dict[nonstd] = std.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the timezones ARE already standard, they should just remap to themselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n",
      "286\n"
     ]
    }
   ],
   "source": [
    "print(len(timezone_dict.keys()))\n",
    "std_dict = {a:a for a in timezone_dict.values()}\n",
    "timezone_dict.update(std_dict)\n",
    "print(len(timezone_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "timezones = pd.DataFrame(tweets_with_originals.user_timezone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "timezones['remap'] = timezones.user_timezone.map(timezone_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_timezone</th>\n",
       "      <th>remap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7529</th>\n",
       "      <td>America/Detroit</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8375</th>\n",
       "      <td>America/Atikokan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>EST</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13177</th>\n",
       "      <td>America/Boise</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13555</th>\n",
       "      <td>America/Boise</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14151</th>\n",
       "      <td>America/Boise</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_timezone remap\n",
       "7529    America/Detroit   NaN\n",
       "8375   America/Atikokan   NaN\n",
       "11313               EST   NaN\n",
       "13177     America/Boise   NaN\n",
       "13555     America/Boise   NaN\n",
       "14151     America/Boise   NaN"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timezones.loc[~timezones.user_timezone.isna() & timezones.remap.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 6 examples are hand-cleaned. The tweets are old, so perhaps they used to be allowed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "timezones.at[[7529, 11313], 'remap'] = 'America/New_York'\n",
    "timezones.at[[13177, 13555, 14151], 'remap'] = 'America/Denver' \n",
    "timezones.at[8375, 'remap'] = 'America/Chicago'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "America/New_York       3746\n",
       "America/Chicago        1932\n",
       "America/Los_Angeles    1211\n",
       "America/Lima            725\n",
       "America/Halifax         494\n",
       "Name: remap, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timezones.remap.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_with_originals['std_user_timezone'] = timezones['remap']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning text for sentiment analysis input. \n",
    "\n",
    "So far we've seen that there are emails, links, and various other text elements that should be replaced in the dataset. \n",
    "\n",
    "* Remove social media elongations like \"loooove\" for \"love\" \n",
    "* Remove ellipses (which have been transformed from \"...\" to \"..\" by the substitution above\n",
    "* Remove links\n",
    "* Remove emails\n",
    "* Remove some abbreviations\n",
    "* Remove @s for mentions\n",
    "* Remove # for hashtags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_with_originals['text_clean'] = tweets_with_originals.text_orig.map(lambda tweet: ''.join(''.join(s)[:2] for _, s in itertools.groupby(tweet)))\n",
    "tweets_with_originals['text_clean'] = tweets_with_originals.text_clean.apply(lambda x: re.sub('\\.{2}', '.', x))\n",
    "tweets_with_originals['text_clean'] = tweets_with_originals.text_clean.str.replace(r'http\\S+', '')# remove hyperlinks\n",
    "tweets_with_originals['text_clean'] = tweets_with_originals.text_clean.str.replace(r'\\S+@\\S+\\s?', '') # remove emails\n",
    "tweets_with_originals['text_clean'] = tweets_with_originals.text_clean.str.replace(r'(\\s\\w+)[.](\\w+)', r'\\1[.] \\2') # try to correct missing space at end of sentence\n",
    "tweets_with_originals['text_clean'] = tweets_with_originals.text_clean.str.replace(r'([2-9])hrs?', r'\\1 hours') # remap abbreviation\n",
    "tweets_with_originals['text_clean'] = tweets_with_originals.text_clean.str.replace(r'@(\\w+) ', r'\\1 ') # remove @s in @mentions\n",
    "tweets_with_originals['text_clean'] = tweets_with_originals.text_clean.str.replace(r'@(\\w+) ', r'\\1 ') # remove # from hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_with_originals.to_csv('tweets_with_originals.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
